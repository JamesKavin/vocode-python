---
title: "Agent Reference"
---

# `AgentConfig`

<ParamField body="initial_message" type="Optional[BaseMessage]">
  The initial message the agent should send at the start of the conversation.
</ParamField>

<ParamField body="generate_responses" type="bool">
  Whether the agent should generate responses continuously (True) or only respond once per human input (False).
</ParamField>

<ParamField body="allowed_idle_time_seconds" type="Optional[float]">
  The maximum number of seconds the agent is allowed to go without speaking before being considered idle.
</ParamField>

<ParamField body="allow_agent_to_be_cut_off" type="bool">
  Whether the human can interrupt the agent while it is speaking.
</ParamField>

<ParamField body="end_conversation_on_goodbye" type="bool">
  Whether the agent should end the conversation when it detects the human saying goodbye.
</ParamField>

<ParamField body="send_filler_audio" type="Union[bool, FillerAudioConfig]">
  Whether to play filler audio (typing noises or phrases like "uh" and "um") when the agent is thinking. Can be configured via FillerAudioConfig.
</ParamField>

<ParamField body="webhook_config" type="Optional[WebhookConfig]">
  Configuration for sending webhooks from the agent.
</ParamField>

<ParamField body="track_bot_sentiment" type="bool">
  Whether to track the sentiment of the agent's responses.
</ParamField>

<ParamField body="actions" type="Optional[List[ActionConfig]]">
  Configuration for custom actions the agent can take, such as making API calls.
</ParamField>


# `LLMAgentConfig`

<ParamField body="prompt_preamble" type="str">
  The preamble text to prepend before the prompt when generating responses. This allows priming the model.
</ParamField>

<ParamField body="expected_first_prompt" type="Optional[str]">
  The prompt to use for generating the first response from the agent. If not provided, the agent will generate a response to the first user message without any initial prompt.
</ParamField>

<ParamField body="model_name" type="str">
  The name of the OpenAI model to use, e.g. "text-curie-001".
</ParamField>

<ParamField body="temperature" type="float">
  The sampling temperature to use for the model. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 for ones with a well-defined answer.
</ParamField>

<ParamField body="max_tokens" type="int">
  The maximum number of tokens to generate in the completion.
</ParamField>

<ParamField body="cut_off_response" type="Optional[CutOffResponse]">
  The response for the agent to give when it is cut off mid-response.
</ParamField>


# `ChatGPTAgentConfig`

<ParamField body="prompt_preamble" type="str">
  The preamble text to prepend before the prompt when generating responses. This allows priming the model.
</ParamField>

<ParamField body="expected_first_prompt" type="Optional[str]">
  The prompt to use for generating the first response from the agent. If not provided, the agent will generate a response to the first user message without any initial prompt.
</ParamField>

<ParamField body="model_name" type="str">
  The name of the OpenAI model to use, e.g. "gpt-3.5-turbo-0613".
</ParamField>

<ParamField body="temperature" type="float">
  The sampling temperature to use for the model. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 for ones with a well-defined answer.
</ParamField>

<ParamField body="max_tokens" type="int">
  The maximum number of tokens to generate in the completion.
</ParamField>

<ParamField body="cut_off_response" type="Optional[CutOffResponse]">
  The response for the agent to give when it is cut off mid-response.
</ParamField>

<ParamField body="azure_params" type="Optional[AzureOpenAIConfig]">
  Configuration when using the Azure OpenAI API.
</ParamField>

<ParamField body="vector_db_config" type="Optional[VectorDBConfig]">
  Configuration for hitting a vector database for retrieval before generating a response.
</ParamField>


# `ChatAnthropicAgentConfig`

<ParamField body="prompt_preamble" type="str">
  The preamble text to prepend before the prompt when generating responses. This allows priming the model.
</ParamField>

<ParamField body="model_name" type="str">
  The name of the Anthropic model to use. e.g. "claude-v1".
</ParamField>

<ParamField body="max_tokens_to_sample" type="int">
  Max tokens to sample for autocompletion stream. Defaults to 200.
</ParamField>


# `ChatVertexAIAgentConfig`

<ParamField body="prompt_preamble" type="str">
  The preamble text to prepend before the prompt when generating responses. This allows priming the model.
</ParamField>

<ParamField body="model_name" type="str">
  The name of the Vertex model to use. e.g. "chat-bison@001".
</ParamField>

<ParamField body="generate_responses" type="bool">
  Whether the agent should generate responses continuously (True) or only respond once per human input (False).
  Google Vertex AI doesn't support streaming, set to False
</ParamField>


# `EchoAgentConfig`

The EchoAgent simply echoes back the user's messages. It does not take any configuration parameters.


# `GPT4AllAgentConfig`

<ParamField body="prompt_preamble" type="str">
  The preamble text to prepend before the prompt when generating responses. This allows priming the model.
</ParamField>

<ParamField body="model_path" type="str">
  The path to the model weights file.
</ParamField>

<ParamField body="generate_responses" type="bool">
  Whether the agent should generate responses continuously (True) or only respond once per human input (False).
</ParamField>


# `LlamacppAgentConfig`

<ParamField body="prompt_preamble" type="str">
  The preamble text to prepend before the prompt when generating responses. This allows priming the model.
</ParamField>

<ParamField body="llamacpp_kwargs" type="dict">
  Additional kwargs to pass to the LlamaCpp model initializer.
</ParamField>

<ParamField body="prompt_template" type="Optional[Union[PromptTemplate, str]]">
  The template to use for formatting the prompt with the conversation history and current user input. Can be a string referring to a built-in template like "alpaca", or a custom PromptTemplate.
</ParamField>


# `InformationRetrievalAgentConfig`

<ParamField body="recipient_descriptor" type="str">
  A description of the intended recipient that the agent is trying to reach.
</ParamField>

<ParamField body="caller_descriptor" type="str">
  A description of who the agent is representing in the call.
</ParamField>

<ParamField body="goal_description" type="str">
  A description of the goal or task that the agent is trying to accomplish, to provide context for the information retrieval.
</ParamField>

<ParamField body="fields" type="str">
  A list of the information fields the agent is trying to collect.
</ParamField>


# `RESTfulUserImplementedAgentConfig`

<ParamField body="respond" type="RESTfulEndpointConfig">
  Configuration for the REST endpoint to hit to get responses from the user implemented agent.
</ParamField>

<ParamField body="generate_responses" type="bool">
  Whether the agent should generate responses continuously (True) or only respond once per human input (False).
</ParamField>
